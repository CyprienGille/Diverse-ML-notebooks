{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Plantnet Classification - Sorbier & Orme\n\n*Andrieu GrÃ©goire & Gille Cyprien*","metadata":{}},{"cell_type":"markdown","source":"## Necessary Imports","metadata":{}},{"cell_type":"code","source":"import os # for folder browsing\nfrom tqdm.notebook import tqdm # progress bars\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Subset, WeightedRandomSampler\nimport torch.nn.functional as F # loss function\nfrom torchvision.datasets import ImageFolder # load our dataset, which has the ImageFolder structure\nfrom torchvision import transforms, models # data augmentation and preprocessing\nimport torchvision.transforms.functional as TF # used in old code for on-fecth transforms\nfrom torchvision.utils import make_grid # to display a batch \n\nfrom PIL import Image # useful for single-image loading\nimport copy # model checkpoints\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.preprocessing import normalize # helps to display the confusion matrix\nimport pandas as pd # used to create submission.csv\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:06.126877Z","iopub.status.busy":"2021-11-28T16:13:06.126196Z","iopub.status.idle":"2021-11-28T16:13:08.531556Z","shell.execute_reply":"2021-11-28T16:13:08.53084Z","shell.execute_reply.started":"2021-11-28T16:13:06.126752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"markdown","source":"Our data for this competition is a subset of the Pl@ntnet dataset, containing 140,256 images from 153 species.","metadata":{}},{"cell_type":"code","source":"data_dir_path = \"../input/polytech-nice-data-science-course-2021/polytech/\"\ntrain_dir_path = data_dir_path + \"train/\"\ntest_dir_path = data_dir_path + \"test/\"","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:08.534308Z","iopub.status.busy":"2021-11-28T16:13:08.53314Z","iopub.status.idle":"2021-11-28T16:13:08.538299Z","shell.execute_reply":"2021-11-28T16:13:08.537658Z","shell.execute_reply.started":"2021-11-28T16:13:08.534268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# All the classes\nclasses = os.listdir(train_dir_path)","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:08.549823Z","iopub.status.busy":"2021-11-28T16:13:08.549554Z","iopub.status.idle":"2021-11-28T16:13:13.743266Z","shell.execute_reply":"2021-11-28T16:13:13.7425Z","shell.execute_reply.started":"2021-11-28T16:13:08.549786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis\n\n\nBefore working with our data, we need to explore it to see if there are any caveats to working with it. First, let us display a few images from the dataset.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(18, 18)) # big figure to have enough space for 16 images\nfor i in range(16):\n    # we pick one image from each of the first 16 classes (os.listdir uses alphabetical order)\n    current_path = train_dir_path + classes[i] + \"/\"\n    current_images = os.listdir(current_path) # all images from the selected class\n    \n    img = Image.open(current_path + current_images[0])\n    \n    plt.subplot(4, 4, i+1) # 4 rows and 4 columns\n    plt.imshow(img)\n    plt.title(f\"Image from class {classes[i]}, size {img.size}\")\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this first step, we can see two things : \n - The images can be different sizes\n - The orientation of the plant is relatively random, which means we can use rotations for data augmentation later.\n\nNow, let us examine the repartition of images between classes.","metadata":{}},{"cell_type":"code","source":"# compute the number of images per class\nnums = {}\nfor plant_num in classes:\n    nums[str(plant_num)] = len(os.listdir(train_dir_path + plant_num))\n\nimg_per_class = pd.DataFrame(nums.values(), index=nums.keys(), columns=[\"Number of images\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display as an histogram the size of each class in our dataset\n\nclass_numbers = [i for i in range(153)]\nplt.figure(figsize=(16, 6))\nplt.bar(class_numbers, [nums[str(i+1)] for i in class_numbers], width=0.4)\nplt.xlim(-2, 154)\nplt.xlabel('Classes')\nplt.ylabel('Quantity of images available')\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:13.746208Z","iopub.status.busy":"2021-11-28T16:13:13.745683Z","iopub.status.idle":"2021-11-28T16:13:14.227195Z","shell.execute_reply":"2021-11-28T16:13:14.226457Z","shell.execute_reply.started":"2021-11-28T16:13:13.746159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that our dataset is heavily unbalanced. This is confirmed when we look at its statistics :","metadata":{}},{"cell_type":"code","source":"img_per_class.describe()","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:14.22911Z","iopub.status.busy":"2021-11-28T16:13:14.228316Z","iopub.status.idle":"2021-11-28T16:13:14.255839Z","shell.execute_reply":"2021-11-28T16:13:14.255045Z","shell.execute_reply.started":"2021-11-28T16:13:14.22907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the graph and at the median number of images in a class, we can see that our dataset has mostly small classes, with around 400 images, but also contains a few plant types that are widely over-represented, with over 5000 images for some.\n\nWe will absolutely have too keep this in mind, because our network will naturally predict the oversized classes more frequently. If real-life (i.e. test) data is also unbalanced in the same way, then it would be fine to leave this bias in our model. However, if the test dataset is balanced, or unbalanced differently than the train dataset, then our model would perform poorly. \n\nMitigating the unbalanced-ness of the train dataset will make our model more robust and accurate (cf Progress), so that is what we will focus on in this next section.","metadata":{}},{"cell_type":"markdown","source":"### Data augmentation and oversampling\n\n","metadata":{}},{"cell_type":"markdown","source":"Our way of dealing with our unbalanced dataset is through a technique called **Oversampling**. \nDuring training, our dataloader will pick from under-represented classes with a higher probability. \nThis is achieved through passing a weighted random sampler to the data loader : in the sampler, each element of the training set has a weight associated with it that translates to how likely the sampler is to fetch this particular element.\n\nTo ensure that our oversampling achieves our goal of balancing the classes, we determine the weights as follows:\n\nFor a sample $x$ that belongs to class $Y$:\n$$\nweight(x) = \\frac{1}{size(Y)}\n$$\n\nThat way, each individual sample in an under-represented class has a higher chance of being fetched, but samples from over-represented classes compensate this by being more numerous.","metadata":{}},{"cell_type":"markdown","source":"However, we now have a new problem. With our oversampling technique, samples from small classes will be used for training multiple times per epoch. This will obvioulsy affect our model's ability to generalise, hence the need for another data processing technique: **data augmentation**.\n\nWe use data augmentation directly on the input of ImageFolder: all images will pass through a set of transforms (cf. comments in the code) before being fed to our model. Some of those transforms have a certain probability of happening, which means that the network will no longer train on the same oversampled images.\n\nAs a sidenote, our validation data does not need to be augmented, but it is easier and cleaner to pass the transforms to the dataset at its creation (before the split). That is not a problem, since the purpose of validation data is merely to give us an indicative measure of how good our model is at generalising.","metadata":{}},{"cell_type":"markdown","source":"We may have fixed the balance in our dataset for training, but our actual dataset still \"physically\" has very small classes. That means that if we are not careful, while performing the split to get training and validation data, we could end up with a class being completely distributed in only the validation set. To avoid this, instead of using pytorch's `random_split` function, we use scikit learn's `train_test_split` and its `stratify` argument.","metadata":{}},{"cell_type":"code","source":"# Data augmentation and normalization for training\n# Just normalization for evaluation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224), # Data augmentation : crop to correct size, using a random part of the image\n        transforms.RandomHorizontalFlip(), # perform a Hflip 50% of the time\n        transforms.RandomVerticalFlip(), # perform a Vflip 50% of the time\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224), # not necessary for square images, can be commented out\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n# The normalization values are defined directly as such in the code of the models we will be using later, so we have to use these ones specifically.\n# Same goes for input size, alexnet vgg and resnet have an input size of 224x224 (this means that we are upscaling some of our images!)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_proportion = 0.8\n\nprint(\"Initializing Datasets and Dataloaders...\")\ndataset = ImageFolder(train_dir_path, data_transforms[\"train\"])\n\n\n# split indices\ntrain_indices, valid_indices, _, _ = train_test_split(\n    range(len(dataset)),\n    dataset.targets,\n    stratify=dataset.targets, # This ensures that each class gets split into train/val\n    test_size=1 - train_proportion\n)\n\n# split le dataset avec les indices\ntrain_split = Subset(dataset, train_indices)\nvalid_split = Subset(dataset, valid_indices)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# As presented above, we compute what weight each class should give to its samples\n# Note : We use the nums dictionnary created earlier, which means the len(class) are those from the original, non-split dataset.\n# This does not affect our sampler however, thanks to the stratify parameter we used during splitting:\n# for all classes, len(class_in_train_set) = train_proportion * len(class_pre_split)\n# Therefore, each class weight would simply be scaled up by 1/0.7, which does would not change anything to the relative magnitudes of the weights.\nclass_weights = {int(class_name):1/class_len for (class_name, class_len) in nums.items()}","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:51.851821Z","iopub.status.busy":"2021-11-28T16:13:51.851582Z","iopub.status.idle":"2021-11-28T16:13:51.860358Z","shell.execute_reply":"2021-11-28T16:13:51.859693Z","shell.execute_reply.started":"2021-11-28T16:13:51.851791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the weight for each sample in the train set.\n# This can take a lot of time, but we cannot easily do it in parallel \n# because order HAS to be consistent between the actual samples in the set and their weights\n\nsample_weights = []\nfor i in tqdm(range(len(train_split))):\n    _, y = train_split.__getitem__(i)\n    sample_weights.append(class_weights[y+1])","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:16:32.6208Z","iopub.status.busy":"2021-11-28T16:16:32.620532Z","iopub.status.idle":"2021-11-28T16:32:40.032415Z","shell.execute_reply":"2021-11-28T16:32:40.031645Z","shell.execute_reply.started":"2021-11-28T16:16:32.62077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 8 # Between 8 and 16 gives the best results without taking *too* long\n\n# num workers = 2 because Kaggle CPUs have two cores (2-core Intel(R) Xeon(R) CPU @ 2.30GHz)\n\n# Creating the sampler to fix our unbalanced dataset\n# Note : We can technically make our training data as big as we want now with this way of sampling\n# for example with \n#               WeightedRandomSampler(sample_weights, 3 * len(sample_weights), replacement=True)\n# but then training takes too long\ntrain_sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n\n\n# Training and validation dataloaders\ntrain_dl = DataLoader(train_split, batch_size=batch_size, sampler=train_sampler, num_workers=2)\nval_dl = DataLoader(valid_split, batch_size=batch_size, num_workers=2)","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:35:58.103661Z","iopub.status.busy":"2021-11-28T16:35:58.103144Z","iopub.status.idle":"2021-11-28T16:35:58.109777Z","shell.execute_reply":"2021-11-28T16:35:58.109122Z","shell.execute_reply.started":"2021-11-28T16:35:58.103624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display one training batch\nfor images, labels in train_dl:\n    _, ax = plt.subplots(figsize=(30, 30))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(make_grid(images, nrow=8).permute(1, 2, 0))\n    break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If we have a GPU, set the device we will be using to gpu\n# using the GPU to host the model and process batches in parallel makes training much faster\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:36:21.987147Z","iopub.status.busy":"2021-11-28T16:36:21.986604Z","iopub.status.idle":"2021-11-28T16:36:21.991384Z","shell.execute_reply":"2021-11-28T16:36:21.990551Z","shell.execute_reply.started":"2021-11-28T16:36:21.987108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper function to move data to the GPU correctly \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:36:22.950595Z","iopub.status.busy":"2021-11-28T16:36:22.949965Z","iopub.status.idle":"2021-11-28T16:36:22.955679Z","shell.execute_reply":"2021-11-28T16:36:22.954618Z","shell.execute_reply.started":"2021-11-28T16:36:22.950554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Network","metadata":{}},{"cell_type":"markdown","source":"For more details on the other models that we tried before settling on the one used below, see the Progress section.\n\nFor our task, we use the pre-trained resnet152 model, which is present directly in the torchvision.models module. Before instantiating it however, we define a few wrapper classes for ease of use and display later.","metadata":{}},{"cell_type":"code","source":"# helper function to score a prediction vector\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\n\n# general model methods\nclass PlantNetModel(nn.Module):\n\n    def batch_training(self, batch):\n        \"\"\"feed a batch to the model, compute and return the cross entropy loss\"\"\"\n        imgs, labels = batch\n        out = self(imgs)\n        loss = F.cross_entropy(out, labels)\n        return loss\n    \n    def batch_evaluation(self, batch):\n        \"\"\"feed a batch to the model, \n        compute the cross entropy loss and the accuracy, \n        and return them in a dict\"\"\"\n        imgs, labels = batch\n        out = self(imgs)\n        loss = F.cross_entropy(out, labels)\n        acc = accuracy(out, labels)\n        # note: we only compute the accuracy during evaluation (and not training) to make training faster\n        return {\"val_loss\": loss.detach(), \"val_accuracy\": acc}\n    \n    def eval_epoch(self, outputs):\n        \"\"\"return the mean loss and accuracy over one epoch\"\"\"\n        batch_losses = [x[\"val_loss\"] for x in outputs]\n        batch_accuracy = [x[\"val_accuracy\"] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()  \n        epoch_accuracy = torch.stack(batch_accuracy).mean()\n        return {\"val_loss\": epoch_loss, \"val_accuracy\": epoch_accuracy}\n    \n    \n    def print_epoch_end(self, epoch, result):\n        \"\"\"Helper end-of-epoch print\"\"\"\n        # Epoch +1 because \"Epoch 0\" doesn't look good\n        print((f\"\"\"Epoch [{epoch+1}], train_loss: {result['train_loss']}, val_loss: {result['val_loss']}, val_acc: {result['val_accuracy']}\"\"\")\n        )","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:36:26.051332Z","iopub.status.busy":"2021-11-28T16:36:26.050502Z","iopub.status.idle":"2021-11-28T16:36:26.062393Z","shell.execute_reply":"2021-11-28T16:36:26.061558Z","shell.execute_reply.started":"2021-11-28T16:36:26.051285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PlantResNet152(PlantNetModel):\n    def __init__(self, feature_extract=True):\n        super().__init__()\n        self.net = self.initialize_model(feature_extract)\n        self.FE = feature_extract\n\n    def forward(self, batch):\n        return self.net(batch)\n\n    def initialize_model(self, feature_extract):\n        \"\"\"Helper function to instantiate the model\n        And freeze weights if feature_extract is set to True\"\"\"\n        model = models.resnet152()\n        # if we are only doing feature extraction, freeze all weights in the model\n        if feature_extract:\n            for param in model.parameters():\n                param.requires_grad = False\n        \n        # input size of the last layer of the model\n        num_ftrs = model.fc.in_features\n        # replacing the last layer with a new one\n        # Note : the new weights thus have require_grad set to true by default\n        model.fc = nn.Linear(num_ftrs, 153)\n\n        return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"During training, we are going to fine-tune the pre-trained weights of resnet152. The training loop is very standard, and saves a checkpoint of the best model for later use.","metadata":{}},{"cell_type":"code","source":"# Function that runs the evaluation phase\n@torch.no_grad()\ndef evaluate(model, val_dl, prog_bar=False):\n    model.eval() # evaluation mode\n    if prog_bar:\n        outputs = [model.batch_evaluation(to_device(batch, device)) for batch in tqdm(val_dl)]\n    else:\n        outputs = [model.batch_evaluation(to_device(batch, device)) for batch in val_dl]\n    return model.eval_epoch(outputs)\n    \n    \ndef full_training(epochs, \n                  lr, \n                  model, \n                  train_dl, \n                  val_loader,\n                  weight_decay=0,\n                  grad_clip=None, \n                  optimizer=torch.optim.SGD\n                  ):\n    \"\"\"Full training routine\n\n    Parameters\n    ----------\n    epochs : [int]\n        [The number of epochs of training]\n    lr : [float]\n        [Initial learning rate]\n    model : [a subclass of PlantNetModel that implements the forward method]\n        [model to train]\n    train_dl : [DataLoader]\n        [training set dataloader]\n    val_loader : [DataLoader]\n        [validation set dataloader]\n    weight_decay : [float], optional\n        [decay coefficient for the weights of the model]\n    grad_clip : [float], optional\n        [if specified, value to which we clip exploding gradients], by default None\n    optimizer : [optimizer], optional\n        [learning algorithm], by default torch.optim.SGD\n\n    Returns\n    -------\n    [list]\n        [losses and accuracy during training]\n    \"\"\"\n    \n    torch.cuda.empty_cache() # make space\n    history = []\n    best_val_acc = 0\n    best_model_wts = copy.deepcopy(model.net.state_dict())\n\n    # listing the weights to update to pass them to the optimizer\n    params_to_update = model.net.parameters()\n    if model.FE:\n        params_to_update = []\n        for _,param in model.net.named_parameters():\n            if param.requires_grad == True:\n                params_to_update.append(param)\n\n    \n    optimizer = optimizer(params_to_update, lr, weight_decay=weight_decay)\n    \n    # Note: we define the scheduler inside this function (unlike the optimizer for example)\n    # because schedulers have very different behaviors (some have args for sched.step(), some need to be called between each batch, etc...)\n    # Note 2 : There are many ways to schedule the learning rate, see the Progress section for more details on our choice\n    # sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, mode=\"max\", verbose=True)\n    sched = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, verbose=True)\n    \n    for epoch in range(epochs):\n        ## Training\n        print(f\"\\nStarted training epoch {epoch+1}...\")\n        model.train() # training mode\n        train_losses = []\n        for batch in tqdm(train_dl):\n\n            batch = to_device(batch, device) # load the batch to the GPU\n            loss = model.batch_training(batch) # train on current batch\n            train_losses.append(loss)\n            loss.backward() # propagate loss\n            \n            # gradient clipping to prevent exploding gradients\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n                \n            optimizer.step() # update weights\n            optimizer.zero_grad() # reset gradient computations for next batch\n\n        ## validation\n        # evaluation routine\n        print(\"Evaluating...\")\n        result = evaluate(model, val_loader, prog_bar=False)\n        # mean loss over all batches\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        # display the end-of-epoch stats\n        model.print_epoch_end(epoch, result)\n        \n        history.append(result)\n        \n        vc = result[\"val_accuracy\"]\n        # sched.step(vc) # for reduce lr on plateau\n        sched.step()\n        if  vc > best_val_acc:\n            best_val_acc = vc\n            print(f\"Saving new best model from epoch {epoch+1} with val_acc of {vc} ...\")\n            best_model_wts = copy.deepcopy(model.net.state_dict())\n            torch.save(best_model_wts, \"best_model.pt\")\n    \n    model.net.load_state_dict(best_model_wts) # give model the best weights encountered during training\n    return history","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:36:38.744383Z","iopub.status.busy":"2021-11-28T16:36:38.743956Z","iopub.status.idle":"2021-11-28T16:36:38.756485Z","shell.execute_reply":"2021-11-28T16:36:38.755666Z","shell.execute_reply.started":"2021-11-28T16:36:38.744347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Controllable parameters\n\n# toggle between fine-tuning and feature extraction\nfeature_extract = False\n\n# Number of epochs\nepochs = 12\n# starting learning rate\nlr = 0.00001\n# Max gradient allowed\ngrad_clip = 5 # just in case there is a bad batch\n# Decay coefficient for regularization\n# weight_decay = 1e-5 # leave commented in case of fine-tuning\n\n# optim = torch.optim.SGD\noptim = torch.optim.Adam \n# optim = torch.optim.AdamW","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = PlantResNet152(feature_extract=feature_extract)\nmodel = to_device(model, device)\n","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:56:57.609086Z","iopub.status.busy":"2021-11-28T16:56:57.60853Z","iopub.status.idle":"2021-11-28T16:56:57.658024Z","shell.execute_reply":"2021-11-28T16:56:57.657139Z","shell.execute_reply.started":"2021-11-28T16:56:57.609048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = [evaluate(model, val_dl, prog_bar=True)] # before any training, store the performance of the model","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:36:40.977807Z","iopub.status.busy":"2021-11-28T16:36:40.977354Z","iopub.status.idle":"2021-11-28T16:41:55.695493Z","shell.execute_reply":"2021-11-28T16:41:55.694803Z","shell.execute_reply.started":"2021-11-28T16:36:40.977771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history # since the weights are initialised randomly, the model does not perform well without training","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history += full_training(epochs, lr, model, train_dl, val_dl, grad_clip=grad_clip, optimizer=optim)","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:41:55.731724Z","iopub.status.busy":"2021-11-28T16:41:55.731328Z","iopub.status.idle":"2021-11-28T16:43:25.643825Z","shell.execute_reply":"2021-11-28T16:43:25.642683Z","shell.execute_reply.started":"2021-11-28T16:41:55.731683Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# very simple plotting of the accuracy and losses through training\nplt.figure(figsize=(15, 5))\nmpl.rcParams[\"axes.titlesize\"] = 12\nmpl.rcParams[\"axes.labelsize\"] = 15\nmpl.rcParams[\"legend.fontsize\"] = 15\nmpl.rcParams[\"lines.linewidth\"] = 3\nmpl.rcParams[\"xtick.labelsize\"] = 12\nmpl.rcParams[\"ytick.labelsize\"] = 12\n\naccuracies = [x['val_accuracy'] for x in history]\nplt.subplot(121)\nplt.plot(accuracies)\nplt.xlabel('Epoch')\nplt.ylabel('Validation Accuracy')\nplt.title('Accuracy by Epoch')\n\ntrain_losses = [x.get('train_loss') for x in history]\nval_losses = [x['val_loss'].cpu() for x in history]\nplt.subplot(122)\nplt.plot(train_losses, 'r')\nplt.plot(val_losses, 'g')\nplt.xlabel('Epoch')\nplt.ylabel('Cross Entropy Loss')\nplt.legend(['Training', 'Validation'])\nplt.title('Loss by Epoch')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Experiments","metadata":{}},{"cell_type":"code","source":"model.eval() # we don't want to train the model anymore","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Confusion matrix","metadata":{}},{"cell_type":"code","source":"# We will use the validation data to compute our confusion matrix\n\ntrue_labels = []\npred_labels = []\nfor batch in tqdm(val_dl):\n    batch = to_device(batch, device)\n    imgs, labels = batch\n    out = model(imgs)\n    _, preds = torch.max(out, dim=1)\n\n    # get the labels out of the tensors\n    true_labels += labels.tolist()\n    pred_labels += preds.tolist()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef disp_confusion_matrix(y_true, y_pred, classes):\n\n    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\n    cm = normalize(cm, axis=1, norm='l1') # for the colormap\n\n    df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n\n    plt.figure(figsize=(120,70))\n    sn.heatmap(df_cm, annot=True, cmap=sn.cubehelix_palette(light=1, as_cmap=True))\n    plt.title('Confusion Matrix',fontdict={'fontsize':20})\n    plt.xlabel('Predicted labels')\n    plt.ylabel('True labels')\n    plt.show()\n\n\ndisp_confusion_matrix(true_labels, pred_labels, dataset.classes)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems that some classes are very rarely correctly predicted. Going back to the histogram at the start of this notebook, it seems that they are all classes with very small cardinality : oversampling and data augmentation has its limits. On top of that, some classes are very often mistaken for another specific class, slightly bigger. Those pairs of classes represent very similar plant types, which the model cannot differentiate. This phenomenon can be mitigated with a weighted loss function, but overall accuracy does not improve along with it (Cf. Progress).","metadata":{}},{"cell_type":"markdown","source":"### Creating challenge submission","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# import the sample submission to imitate its structure\norig_sub = pd.read_csv(f\"{data_dir_path}sample_submission.csv\")","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:58:16.217614Z","iopub.status.busy":"2021-11-28T16:58:16.216898Z","iopub.status.idle":"2021-11-28T16:58:16.232798Z","shell.execute_reply":"2021-11-28T16:58:16.232123Z","shell.execute_reply.started":"2021-11-28T16:58:16.217576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pred(img_name):\n    \"\"\"Get the prediction for a single image of the test folder\"\"\"\n    img = Image.open(test_dir_path + img_name)\n    in_tr = data_transforms[\"val\"] # input transform\n    img = in_tr(img)\n    \n    img_batch = img.unsqueeze(0).cuda() # add the batch dimension\n    out = model(img_batch)\n    _, pred_class = torch.max(out, dim=1)\n    return pred_class","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:58:16.535969Z","iopub.status.busy":"2021-11-28T16:58:16.53536Z","iopub.status.idle":"2021-11-28T16:58:16.541654Z","shell.execute_reply":"2021-11-28T16:58:16.540629Z","shell.execute_reply.started":"2021-11-28T16:58:16.535932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(columns=[\"image_name\", \"class\"])\n\nfor i, img_name in enumerate(tqdm(orig_sub[\"image_name\"])):\n    submission.at[i, \"image_name\"] = img_name\n    submission.at[i, \"class\"] = dataset.classes[pred(img_name).item()]","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:58:17.260086Z","iopub.status.busy":"2021-11-28T16:58:17.259482Z","iopub.status.idle":"2021-11-28T16:58:23.48627Z","shell.execute_reply":"2021-11-28T16:58:23.48524Z","shell.execute_reply.started":"2021-11-28T16:58:17.260051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-24T17:03:20.568904Z","iopub.status.idle":"2021-11-24T17:03:20.569707Z","shell.execute_reply":"2021-11-24T17:03:20.569491Z","shell.execute_reply.started":"2021-11-24T17:03:20.569464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Progress","metadata":{}},{"cell_type":"markdown","source":"In this section we will go through our different reasonings and tests behind all the changes that lead to the final state of the notebook above, chronologically.","metadata":{}},{"cell_type":"markdown","source":"### First trials : ConvNet\n\nAt first, to get a baseline for what a \"good\" performance should look like, we wanted to try a very simple run of a very simple home-made convolutional network.\n\n|*First trial*| val_acc : 0.42 |\n|---|:---:|\n|Architecture|ConvNet (see Annex)|\n|input_size|183x183|\n|dropout|0.1|\n|train_proportion|0.9|\n|splitting method|torch.utils.data.random_split|\n|batch_size|32|\n|epochs|5|\n|lr|0.01|\n|grad_clip|No|\n|weight_decay|No|\n|optimizer|Adam|\n|lr scheduler|No|\n\nNote: the input size is 183x183 because we noticed that that was the smallest size present in the dataset, and we wanted to avoid upscaling images if we did not have to.\n\nThis first run and the experiments we did around it allowed us to establish a few things:\n - Adam seemed to be the best optimizer, with the quickest convergence and best scores.\n - 0.01 was the starting learning rate with the best results.\n - However, the model's performance started stagnating very quickly: we need to adjust the learning rate after a few epochs.\n - We could afford to reduce the training proportion without losing too much accuracy, to speed up training.\n - A batch size of 8 or 16 actually gave better results than higher ones, while only adding a few minutes to training.\n - A probability of dropout of 0.1 yielded the best results.\n - There is something wrong with our submission : our validation accuracy is 0.42, but our test accuracy is around 0.005.\n\nThis last point would be our main issue for a while. At the time, our main hypothesis was that the test set had a very different class distribution than the training dataset, on purpose, to force us to have very robust models. We did the EDA part of this notebook, and thought that maybe the test set was balanced. In fact, there were a few classes that our current model never predicted (the small ones), so we decided to put our hypothesis to the test.","metadata":{}},{"cell_type":"markdown","source":"### Main trials I : ResNet with no Data Augmentation\n\nTo see if our submission accuracy issue really came from a dataset difference, we decided first to try to improve our model's performances, to see whether the test accuracy followed the validation accuracy or not.\n\nWith that goal in mind, we used a much more complex home-made model: a residual convolutional network. Seeing how similar some of the classes were, we wanted to make sure that we had a model complex enough to achieve our complex classification task.\n\nSo, after a lot of (some failed, some successful) runs, tuning of the parameters to get the best validation accuracy possible without modifying the dataset in any way, we ended up with this state of our model:\n\n|*Result of the main trials*| val_acc : 0.56 |\n|---|:---:|\n|Architecture|ResNet (see Annex)|\n|input_size|183x183|\n|dropout|0.1|\n|train_proportion|0.75|\n|splitting method|sklearn.model_selection.train_test_split|\n|batch_size|16|\n|epochs|15|\n|lr|0.01|\n|grad_clip|1|\n|weight_decay|1e-5|\n|optimizer|Adam|\n|lr scheduler|StepLR with step_size = 4|\n\nThose several runs and the experiments we did around them allowed us to establish more things:\n - Making our ResNet deeper did not have any noticeable impact on either validation accuracy nor test accuracy.\n - Using more epochs, or doing a warm restart on our trained model (i.e. recycling through the learning rate) did not improve the validation accuracy : it seemed like we reached a cap.\n - Keeping train proportion between 0.75 and 0.8 was the best tradeoff between performance and training speed.\n - As mentioned above in the code, using train_test_split instead of random_split guaranteed properly stratified train and val datasets.\n - Using StepLR performed better than both ReduceLROnPlateau using the validation accuracy, and CycleLR (increasing batch-by-batch the lr until around a third of the epochs, then going back down to a very low value).\n - Due to the residual nature of our network, some training loops displayed wild divergence of the network because of the exploding gradient problem. Using gradient clipping with values between 1 and 5 fixed that issue without slowing down convergence.\n - Weight decay improved very marginally the performances during training, but had no visible drawback.\n - Kaggle runs and sessions time out after 9 hours of runtime, so it is wise to save checkpoints of halfway-trained models of we intend to use them afterwards.\n - Our submission definitely has an issue, because it does not seem to improve even though the model itself has a better validation accuracy.\n\nAt this point, our conclusion was that we had to balance our training dataset one way or another. We blamed the inbalance in the dataset for both our validation accuracy cap, and our abysmal test score.","metadata":{}},{"cell_type":"markdown","source":"### Main trials II : ResNet with Oversampling, and Data Augmentation\n\nWe had a few options to balance our dataset. \n\nThe first one, the brute-force one, was to physically save augmented images (rotated by some multiple of 90 degrees, lightly blurred, etc) to increase the size of each class until it reached a given threshold. There were several issue with this approach. First, determining the threshold wouldn't be easy: some classes only have around 100 samples, while the largest one has around 6000. Data augmentation has its limits before it starts to be detrimental to , and it would be unreasonable to try to increase each class to a size even a third as large as the largest one. Second, the input folder on Kaggle is read-only, and it would not be practical to have our augmented dataset be split between the input and output folders (especially since we would have to re-do the augmentation everytime, seeing as the output folder is empty at the start of every session). In short, this solution seems to be both limited in effect and cumbersome in practice. (Cf. Annex : Manual dataset balancing)\n\nThe second one was to leave the dataset intact, and use a weighted sampler to oversample the under-represented classes, and conversely undersample the over-representated classes (Cf. comments about that technique above in the code). Paired with random data augmentation on every sample, this proved to be quite effective.\n\nThe third one was to use a weighted loss, where smaller classes would have a greater impact on the loss than larger ones. Whether paired with oversampling and data augmentation or not, this actually worsened our score. However, upon inspection of the confusion matrix, it had the merit of making very small classes have many more correct predictions than before. Sadly, this improvement was largely outweighed by how worse predictions on large classes were doing.\n\nAfter even more 7-hour-long runs, tuning our new parameters, we ended up with this state of our experiments:\n\n|*Result of the main trials with oversampling and DA*| val_acc : 0.62 |\n|---|:---:|\n|Architecture|ResNet (see Annex)|\n|input_size|183x183|\n|dropout|0.1|\n|train_proportion|0.8|\n|splitting method|sklearn.model_selection.train_test_split|\n|Augmentation technique|All classes, random hflip, vflip, blur (Cf. Annex: RandomFlipsTransform)|\n|len(train_dataloader)|2*len(train_split)|\n|batch_size|16|\n|epochs|10|\n|lr|0.01|\n|grad_clip|1|\n|weight_decay|1e-5|\n|optimizer|Adam|\n|lr scheduler|ReduceLROnPlateau with patience=2|\n\nThose several runs and the experiments we did around them allowed us to conclude some points:\n - At first, we tried to perform augmentation only on small classes, using a custom dataset (Cf. Annex: SmartAugmentationDataset). This gave worse results than simply performing augmentation on all samples of the dataset.\n - Since we are using a random sampler, we can ask it to give us as many sample as we want using the given weights. Training on three times our dataset takes too much time, but using twice as much samples did improve the accuracy by around 2%.\n - However, this technique constrains our number of epochs because it effectively doubles training times. \n - Because of how WeightedRandomSampler works, we have to compute the weight of each sample in the training split. This is not parallelizable (Cf. code comments above) and takes significant time.\n - The largest number of epochs we could thus use is 10.\n - This time, because we can no longer train for that many epochs, using ReduceLROnPlateau leads to better results.\n\nAround that time, we discovered that altough balancing the training data did improve the model's performances, the low scores of our submissions actually came from a witless oversight in how we produced the predicted class. In short, the number given as the output of our model was not directly a class, it was the index of the class in an alphabetically ordered list of all classes. Many thanks to M. Bahl for pointing that out on the course's slack.\n\nAfter fixing this issue, we now had to deal with a new, quadruple-faced problem. The deadline was approaching, we were running out of GPU time for our last week, the accuracy of our model seemed to have reached a cap, and (almost) everyone else on the leaderboard was sitting between 0.8 and 0.9 accuracy on the leaderboards.","metadata":{}},{"cell_type":"markdown","source":"### Final runs : Pretrained Models\n\n\nSeeing everyone with such similar scores, with some teams having achieved their best score very early in the competition, we deduced that our colleagues were most likely using pre-trained models.\n\n*Why didn't we use pretrained models earlier?*\nTo be honest, there are probably three main reasons:\n - We wanted to see how good a custom, home-made model could get;\n - We spent around two weeks without knowing how our model would actually perform on the leaderboard;\n - All of the torchvision models are trained on ImageNet, which has 1000 very diverse classes (animals, objects, vegetables, landscapes...) : they are very general models. Our task is much more specific: for example, an overwhelming majority of images in our dataset is mostly green. Therefore, we originally thought that the dataset chosen for this competition was too specific to yield good results on general-purpose, pre-trained models. One of us having a background in NLP led us to be wary of that specificity in our task.\n\nBut we were wrong, because pre-trained models performed really, well, in smaller runtimes, than our ResNet. We first tried alexnet with and without oversampling, fine-tuning it or doing feature extraction, and fine-tuning with oversampling yielded better results. (e.g. Alexnet, no oversampling, feature extraction : 0.71. Alexnet, with oversampling, feature extraction : 0.73.)\n\n\nWe then fine-tuned resnet18, resnet152, and vgg19 (with batch normalization), with all of the techniques mentioned above (oversampling, data augmentation, gradient clipping...).\n\nIn the end (having no more GPU time nor submissions available), the best result was achieved by:\n\n|*Final performance*| val_acc: 0.82, test score: 0.81 |\n|---|:---:|\n|Architecture|resnet152 (fine-tuning)|\n|input_size|224x224|\n|train_proportion|0.8|\n|splitting method|sklearn.model_selection.train_test_split|\n|Augmentation technique|All classes, random hflip, vflip, resizing crop|\n|len(train_dataloader)|len(train_split)|\n|batch_size|8|\n|epochs|11|\n|lr|0.00001|\n|grad_clip|5|\n|weight_decay|No|\n|optimizer|Adam|\n|lr scheduler|StepLR with step_size=4|\n\nKey points:\n - The input size is determined by the architecture of resnet152. This leads to the upscaling of some of the images in our dataset, something we never experimented with (Cf. Conclusion)\n - Since we did not have much computing time remaining, we never tried to use more than len(train_split) samples through WeightedRandomSampler.\n - Fine-tuning required a much smaller learning rate than before.\n - Weight decay deteriorated the performance of our model, as it was changing the pre-trained weights.\n - StepLR became the best optimizer again, because ReduceLROnPlateau never activated due to marginal improvements being made every epoch.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nThis competition was very enriching, as it covered a lot of aspects of data science: we had to choose a model, we had an unbalanced dataset... If we had to do things differently, or continue working on this project in the future, we would probably focus on the following points:\n - Try larger input sizes with our ResNet: upscaling does not seem to be a problem one should worry about too much.\n - Implement model ensembling. This technique is famous for its use in Kaggle competitions, and could be performed in a number of ways. Averaging over different resnet sizes, over alexnet vgg and resnet152, over our ResNet trained on k-folds of our dataset (cross-validation then model ensembling), or even both at the same time.\n - Look at better, more recent pre-trained models, within torchvision or outside.","metadata":{}},{"cell_type":"markdown","source":"# Annex","metadata":{}},{"cell_type":"markdown","source":"## Previous Models","metadata":{}},{"cell_type":"code","source":"# convolution block avec BatchNormalization\ndef ConvBlock(in_channels, out_channels, pool=False, kernel_size=3, padding=1, stride=1, pooling_kernel_size=3):\n    \n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride),\n             nn.BatchNorm2d(out_channels),\n             nn.ReLU(inplace=True)]\n    if pool:\n        layers.append(nn.MaxPool2d(pooling_kernel_size))\n    return nn.Sequential(*layers)\n\n# simpler net\nclass ConvNet(PlantNetModel):\n    def __init__(self, in_channels, n_classes):\n        super().__init__()\n        \n        self.conv1 = ConvBlock(in_channels, 64) # 3x183x183\n        self.conv2 = ConvBlock(64, 128, pool=True) # 128x61x61\n        \n        \n        self.conv3 = ConvBlock(128, 256, pool=True) # 256 x 20 x 20\n        self.conv4 = ConvBlock(256, 512, pool=True) # 512 x 6 x 6\n        self.conv5 = ConvBlock(512, 512, pool=True) # 512 x 2 x 2\n        \n        self.dropout = nn.Dropout(p=0.1) \n        \n        self.classif = nn.Sequential(nn.MaxPool2d(2),\n                                     nn.Flatten(),\n                                     nn.Linear(512, n_classes))\n        \n    def forward(self, batch):\n        out = self.conv1(batch)\n        out = self.conv2(out)\n        out = self.dropout(out)\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.dropout(out)\n        out = self.conv5(out)\n        out = self.classif(out)\n        return out\n\n# resnet architecture\nclass ResNet(PlantNetModel):\n    def __init__(self, in_channels, n_classes):\n        super().__init__()\n        \n        self.conv1 = ConvBlock(in_channels, 32) # 3*183*183\n        self.conv2 = ConvBlock(32, 64, pool=True) # 64x61x61\n        self.conv3 = ConvBlock(64, 128, pool=True) # 128 x 20 x 20\n        self.res1 = nn.Sequential(ConvBlock(128, 128), ConvBlock(128, 128), ConvBlock(128, 128))\n\n        self.conv4 = ConvBlock(128, 256, pool=True) # 256 x 6 x 6\n        self.conv5 = ConvBlock(256, 256, pool=True, pooling_kernel_size=2) # 256 x 3 x 3\n        self.res2 = nn.Sequential(ConvBlock(256, 256), ConvBlock(256, 256), ConvBlock(256, 256))\n        \n        self.classif = nn.Sequential(nn.MaxPool2d(3),\n                                     nn.Flatten(),\n                                     nn.Linear(256, n_classes))\n        \n        self.dropout = nn.Dropout(p=0.1)\n        \n    def forward(self, batch):\n        out = self.conv1(batch)\n        out = self.conv2(out)\n        out = self.conv3(out)\n        out = self.res1(out) + out\n        out = self.dropout(out)\n        out = self.conv4(out)\n        out = self.dropout(out)\n        out = self.conv5(out)\n        out = self.dropout(out)\n        out = self.res2(out) + out\n        out = self.classif(out)\n        return out","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:36:27.247002Z","iopub.status.busy":"2021-11-28T16:36:27.246394Z","iopub.status.idle":"2021-11-28T16:36:27.25938Z","shell.execute_reply":"2021-11-28T16:36:27.258671Z","shell.execute_reply.started":"2021-11-28T16:36:27.246947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Code that we no longer use","metadata":{}},{"cell_type":"markdown","source":"### \"Smart\" Data Augmentation","metadata":{}},{"cell_type":"code","source":"# when getting from the under-represented classes, transform (to avoid always giving the same images)\n\nclass SmartAugmentationDataset(ImageFolder):\n    def __init__(self, path, transform=None, aug_class_id=None):\n        super().__init__(path, transforms.Compose([transforms.Resize([183, 183]), transforms.ToTensor()]))\n        self.transform = transform\n        self.aug_class_id = aug_class_id\n\n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, index): \n        x, y = super().__getitem__(index)\n        \n        if self.aug_class_id is not None and self.transform is not None: # if the class needs augmentation, and we provided augmentation transforms\n            if y in self.aug_class_id:\n                x = self.transform(x)\n        \n        return x, y","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:14.25749Z","iopub.status.busy":"2021-11-28T16:13:14.25715Z","iopub.status.idle":"2021-11-28T16:13:14.264735Z","shell.execute_reply":"2021-11-28T16:13:14.264021Z","shell.execute_reply.started":"2021-11-28T16:13:14.25745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# figure out the indices of classes that need augmentation\naug_class_id = []\n\nfor id in range(1, 154):\n    n_img = nums.get(str(id))\n    if n_img < 927: # 75% of classes (q3)\n        aug_class_id.append(id)\n","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:14.266876Z","iopub.status.busy":"2021-11-28T16:13:14.266173Z","iopub.status.idle":"2021-11-28T16:13:14.274735Z","shell.execute_reply":"2021-11-28T16:13:14.274014Z","shell.execute_reply.started":"2021-11-28T16:13:14.266838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data augmentation transform\nclass RandomFlipsTransform:\n\n    def __init__(self, identity_proba=0.5) -> None:\n        self.identity_proba = identity_proba\n\n    def __call__(self, img):\n        if torch.rand(1) < self.identity_proba:\n            return img\n        return self.change_image(img)\n\n    def change_image(self, img):\n        p = torch.rand(1)\n        if p < 0.25:\n            return TF.rotate(img, 90)\n        elif p < 0.5:\n            return TF.rotate(img, 180)\n        elif p < 0.75:\n            return TF.rotate(img, 270)\n        return TF.gaussian_blur(img, 3)\n        \n\n\naug_transforms = RandomFlipsTransform(0.66)","metadata":{"execution":{"iopub.execute_input":"2021-11-28T16:13:14.276653Z","iopub.status.busy":"2021-11-28T16:13:14.27613Z","iopub.status.idle":"2021-11-28T16:13:14.2873Z","shell.execute_reply":"2021-11-28T16:13:14.286463Z","shell.execute_reply.started":"2021-11-28T16:13:14.276614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Local Manual Data Augmentation","metadata":{}},{"cell_type":"markdown","source":"Just to see if it would actually improve performance, we implemented the manual data augmentation on one of our computers. It was not worth it.","metadata":{}},{"cell_type":"code","source":"# This code cannot work in a Kaggle notebook due to the read-only nature of the input folder\n# also it permanently modifies your input folder, so it should not be ran by default\n# also also, throwing out data is very rarely good practice\n\nif False: \n    import os\n    from PIL import Image, ImageFilter\n    from tqdm.notebook import tqdm\n\n    data_dir = \"../input/polytech-nice-data-science-course-2021/polytech/train/\"\n\n    classes = os.listdir(data_dir)\n\n\n    nums = {}\n    aug_class_id = []\n    big_class_id = []\n    for plant_id in classes:\n        n = len(os.listdir(data_dir + \"/\" + plant_id))\n        nums[str(plant_id)] = n\n        if n < 927:  # q3\n            aug_class_id.append(plant_id) # all classes with less than 927 samples will see their size get tripled\n        elif n > 3000:\n            big_class_id.append(plant_id) # all classes with more than 3000 samples will see their size capped to 3000\n\n\n    # Augmentation : save a a rotated by 90 degrees version and a slightly blurred version of each image\n    # (for classes that need augmentation)\n    for plant_id in aug_class_id:\n        print(f\"Augmenting class {plant_id}...\")\n        current_path = f\"{data_dir}/{plant_id}/\"\n        current_images = os.listdir(current_path)\n        for img_path in tqdm(current_images):\n            if img_path[-3:] == \"jpg\":\n                img = Image.open(current_path + img_path)\n                tr1 = img.rotate(90, expand=True)\n                tr2 = img.filter(filter=ImageFilter.GaussianBlur(radius=1))\n                tr1.save(current_path + img_path[:-4] + \"r.jpg\")\n                tr2.save(current_path + img_path[:-4] + \"b.jpg\")\n        print(f\"Class {plant_id} now has size {len(os.listdir(current_path))}\\n\")\n\n\n    # Remove images from the large classes to bring them back to 3000 images\n    for plant_id in big_class_id:\n        print(f\"Reducing class {plant_id}...\")\n        current_path = f\"{data_dir}/{plant_id}/\"\n        orig_imgs = os.listdir(current_path)\n        orig_s = len(orig_imgs)\n        to_remove = orig_s - 3000\n        for i in tqdm(range(to_remove)):\n            os.remove(current_path + orig_imgs[i])\n","metadata":{},"execution_count":null,"outputs":[]}]}